Love Simulations|the_facility.jpg|I watched a ted talk and didn't believe the speaker. Spoiler: she was right|Because Love and Statistics Go Hand-In-Hand
|PARAGRAPH|I watched a TED talk a few years ago about how we can use statistics to theoretically find the person statistically most likely
to be someone's soul mate. The algorithm described in the talk basically said that any partners you have while under the age of 23 should be considered as "tests" 
and after that point, you should settle down with the next person you find that you like more than any of the people you previously dated. 
It's a very cold and calculated system, but it has a basis in statistical analysis. Does it actually work though?
|PARAGRAPH|Given that June is supposed to be the marriage month, I decided to create a simulation of different systems to see if this system really is the best, and if it isn't, seek out a system which is better.

|BREAK|PARAGRAPH|To start, I needed a population to work on. Generating a group of simulated people was as easy as randomly generating features. Each of these entities I have randomly generated takes a given number of stats which are scored from 0 to 100, 
and they are given a desired set of statistics for their partner. The chances are that very few, if not none, of these entities will actually have a perfect partner, but a close value is possible. 
When an entity selects a partner, they are given a compatability score based on how much the statistics of each of the entities differ from their partners desires. This is a two-way system, since a person needs to be liked back in order for a relationship to work.
This means that "soul mates" would have an error of 0, and "mortal rivals" would have an error of 200 times the number of stats. For the bulk of the experiments, I have decided to create 100,000 entities, each with 12 stats. These numbers are abitrary.

|BREAK|PARAGRAPH|The first experiment I have run is a random baseline. Each entity picks a partner at random and once they are all linked to another partner, we figure out what the average error is. We're running this simulation from the perspective of a single entity
with random stats who is searching for a partner, and we'll be doing it this was for all experiments.
|PARAGRAPH|I ran the random baseline experiment 1000 times and found the average error to be 807. This is not a good score, as it implies a difference of around 33 on every single stat between parties. We don't really have any context as to what that means, but we're
aiming for the lowerst number we can, so we definitely want to do better. Obviously a random system isn't going to give us a good value - that's obvious. Picking the first person you see on the street and going "alright, we're married now" is never going to work (go ahead and prove me wrong), 
so we need to try simulating some more complex systems.

|BREAK|PARAGRAPH|Before running a more complex system, however, we should generate a secondary baseline in the form of a brute-force search. Our entity will have its compatability with every other entity checked to find out which entity it is most compatible with. 
Since I'm not generating perfect soulmates, and rather going for entirely random values, I expect that the brute force search will give us a very low number, but likely not quite the 0 we would realistically want.
|PARAGRAPH|The brute force search gives us an error of 61. This means that each of the stats of the stats is off by only 2.5 on average each way. Given that we generated 100,000 entities, each with 12 stats, each with a compatability score out of 200, there are 24 million combinations, so a difference of only 61 seems insane.
|PARAGRAPH|I think we should take this moment to consider thhe context of our scores. It looks like 61 is the best possible score we can get, so where do we draw the line at "good enough"? As far as I can tell, the TED talk we're getting our algorithm from is only checking that we're doing better than
a random choice of partner, but I don't that that'd be particularly hard to do. This is why I propose we introduce a time restriction. In the TED talk it is mentioned that the optimal ages to settle down and get married are between 23 and 30. A study (Klein and O'Brien, 2018) said that it takes about 6 months before you
know if someone is marriage material. Say you start dating seriously at the age of 16, you only have 28 chances before you're 30, so we'll only give our entity 28 steps before forcing them to settle down with the next available person - regardless of their error rate. 

|BREAK|PARAGRAPH|Onto some proper experiments then. We began by generating a thresholding system. Our entity will randomly select another entity and check its score, ignoring the entity if their compatability isn't good enough. The threshold is initially set at 800, though this definitely needs to be changed as we continue going. 
The system with a threshold of 800 gives is an average error of 790. Because the error is lower than the threshold, we can likely lower this threshold. By lowering the threshold, the average error should lower, but only to a point. After a certain point, the error will starts going up again, as the entity is not able to find a
 good option before the time limit expires. To find an optimal threshold, we can ran a quick experiment to test different threshold value to find an optimal.
 |IMAGE|threshold_graph.png|
|PARAGRAPH|The graph here shows the results of the experiment. We ran the experiment with thresholds as a multiple of the baseline. With a threshold at 0.1, (that meaning a threshold of 80) the system is almost always unable to find a partner for a given entity, 
so the error doesn't change from that of the random system - every time we try, we fail and just have to go with a random. At a threshold of 0.3, we find the lowest error. With a threshold of 0.3 (that being a threshold of around 240 error), the error rate drops to 663 on average. This means that the thresholding system is 
able to find better partners than the random system does, but the error rate is still nowhere near the optimal values. 240 is still such a low chance of being found that the average is brought up by the times we don't find anything.
|PARAGRAPH|Anything above 0.3 causes our value to go up, implying that the threshold is too high, and the system just isn't very good at narrowing down options. I would have expected a more gradual increase after that point, but due to the distance between each of the thresholds being around 80 units, the sudden ups and downs are somewhat expected. 
Really, this system doesn't make much sense in the real world anyway - as a person roaming the world looking for love, you don't get to run experiments to find the best threshold for your partner. We can potentially fix this using Simulated Annealing.

|BREAK|PARAGRAPH|Oh boy simulated annealing is my favourite - I've only written several hundred annealing systems in my lifetime and we obviously need more. Annealing refers to a simple cooling rate which allows a system to relax or tighten its constraints as time goes by. An annealing system in thie case would act just like our thresholding
system, starting with the threshold just below the 0.3 that we found best in our static threshold system, and gradualling increasing the threshold until we find a partner who fits. By starting our threshold at 0.2 and increasing by 0.025 each step, we'll end up with a threshold of 0.9 (or 720) on the 28th step before we run out of time. Realistically,
this should be plentry of time and plenty of steps to find someone.
|PARAGRAPH|Running this sytem got us an average error of 661, which is better than running without the annealing, but it's still not great. In the end, just over 58% of the runs we did were able to find a partner in the allotted time, so there's still plenty ways to go.

|BREAK|PARAGRAPH|Let's look at what we actually came here for then - the algorithm described in the TED talk. I can not find the talk again, so some of my details are likely wrong, but from what I remember, the system calls for a given duration of testing of partners, and after that time is over, 
the entity will take the next partner it finds that has a better compatability than any of the previous entities it dated. The "training" period runs from the age of 16 to the age of 23, so a total of 14 of the 28 steps.
|IMAGE|algorithm_graph.png|
|PARAGRAPH|This graph shows the results of this system across a selection of different numbers of testing partners. As can be seen, even with just 5 test partners, this system is significantly more effective at finding an optimal partner than any of our other experiments. With only 5 partners, an error of 430 is found,
and the system only gets better as more testing is performed. The system gradually plateaus as we test on more entities before settling down, finding an error of only 91 when the entities test on 50 other entities in the "non-marriage phase". 50 is of course far more than we'd be able to fit into our testing period, but given that
we don't need necessarily need to spend 6 months with each testing entity, we could probably fit more in. Nonetheless, an error of 240 after 14 training entities is a great result.
|BREAK|PARAGRAPH|So what can we conclude from this? Yes, the algorithm in the TED talk is the best algorithm for finding a lover, but even though the stats and simulations are correct, would it work in real life? Probably not. There are going to be many other external factors that will create an impact on your result.
Firstly, can you actually attach a numerical value to a romantic encounter? Secondly, are there otehr stats that can't be measured in something like this? People are known to change the way they act around certain people - this system doesn't account for change at all, and there is definitely a lot of change between 16 and 30.
Thirdly, is there going to be a stigma about the guy who statistically analyses everyone they date?
|PARAGRAPH|I think there are definitely other systems that could be implemented that would generate better results - for example combining the simulated annealing with the TED talk system. If we get the value we recieve at the end of the testing and combine it with our annealing system, we have the capacity to get a lower value.
Say our best testing entity gives us a value of 240, we could then start looking for people with a value of 200. Slowly we can anneal our value and make it higher and higher until we find someone who fits.
|END|